{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=['Rishab likes pizza', 'I love gaming', 'Divyansh loves coding','NLP is fun','Chicken is tasty','I love running','Usain runs fast']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Featurize ,Tokenize and labelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffa2dda0f5c4ed79d9b74c20fcba048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary={} # for storing word to index mapping\n",
    "inputs=[] # for storing the input as indexes\n",
    "\n",
    "for sentences in tqdm_notebook(corpus):\n",
    "    sentence_indexes=[]\n",
    "    \n",
    "    sentences=sentences.split()\n",
    "    \n",
    "    for word in sentences :\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(vocabulary)\n",
    "        \n",
    "        sentence_indexes.append(vocabulary[word])\n",
    "        \n",
    "    inputs.append(sentence_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab to index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rishab': 0, 'likes': 1, 'pizza': 2, 'I': 3, 'love': 4, 'gaming': 5, 'Divyansh': 6, 'loves': 7, 'coding': 8, 'NLP': 9, 'is': 10, 'fun': 11, 'Chicken': 12, 'tasty': 13, 'running': 14, 'Usain': 15, 'runs': 16, 'fast': 17}\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenized and index inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 10, 13], [3, 4, 14], [15, 16, 17]]\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to feature sequences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [sentences[:-1] for sentences in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [3, 4], [6, 7], [9, 10], [12, 10], [3, 4], [15, 16]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[sentences[-1] for sentences in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 8, 11, 13, 14, 17]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to long tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes=torch.LongTensor(prefixes)\n",
    "labels=torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 3,  4],\n",
       "        [ 6,  7],\n",
       "        [ 9, 10],\n",
       "        [12, 10],\n",
       "        [ 3,  4],\n",
       "        [15, 16]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  5,  8, 11, 13, 14, 17])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the neural language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>step by step building of nlm model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_language_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dimension,hidden_dimension,\n",
    "                 window_size,vocab_size):\n",
    "        \n",
    "        super(simple_language_model,self).__init__()\n",
    "        \n",
    "        self.embedding_layer=nn.Embedding(vocab_size,embedding_dimension)\n",
    "        \n",
    "        self.hidden_layer1=nn.Linear(embedding_dimension * window_size, hidden_dimension)\n",
    "        \n",
    "        self.hidden_layer2=nn.Linear(hidden_dimension,int(hidden_dimension/2))\n",
    "        \n",
    "        self.output_layer=nn.Linear(int(hidden_dimension/2),vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self,inp):\n",
    "        \n",
    "        batch_size,window_size=inp.size()\n",
    "        \n",
    "        embeddings=self.embedding_layer(inp)\n",
    "        \n",
    "        print(embeddings.size())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_model_test=simple_language_model(embedding_dimension=5,hidden_dimension=14,\n",
    "                               window_size=2,vocab_size=len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each of the n sentences, of prefix size k and embedding dimension e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape is (n,k,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "nlm_model_test(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_language_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dimension,hidden_dimension,\n",
    "                 window_size,vocab_size):\n",
    "        \n",
    "        super(simple_language_model,self).__init__()\n",
    "        self.embedding_dimension=embedding_dimension\n",
    "        self.hidden_dimension=hidden_dimension\n",
    "        \n",
    "        self.embedding_layer=nn.Embedding(vocab_size,embedding_dimension)\n",
    "        \n",
    "        self.hidden_layer1=nn.Linear(embedding_dimension * window_size, hidden_dimension)\n",
    "        \n",
    "        self.hidden_layer2=nn.Linear(hidden_dimension,int(hidden_dimension/2))\n",
    "        \n",
    "        self.output_layer=nn.Linear(int(hidden_dimension/2),vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self,inp):\n",
    "        \n",
    "        batch_size,window_size=inp.size()\n",
    "        \n",
    "        embeddings=self.embedding_layer(inp)\n",
    "        \n",
    "        #concatenate the prefix embeddings\n",
    "        # n x k x e => n *(k*e)\n",
    "        #7 x 2 x 5  => 7 * 10\n",
    "        \n",
    "        flattened_embeddings= embeddings.view(batch_size,window_size * self.embedding_dimension)\n",
    "        \n",
    "        print(f'flattened_embeddins size : {flattened_embeddings.size()}')\n",
    "        print('-'*50)\n",
    "        print(embeddings[0].shape)\n",
    "        print(embeddings[0])\n",
    "        print('-'*50)\n",
    "        print(flattened_embeddings[0].shape)\n",
    "        print(flattened_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_test_2=simple_language_model(embedding_dimension=5,hidden_dimension=14,\n",
    "                               window_size=2,vocab_size=len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_embeddins size : torch.Size([7, 10])\n",
      "--------------------------------------------------\n",
      "torch.Size([2, 5])\n",
      "tensor([[ 0.0125, -0.4433,  0.3469,  0.6609,  1.0673],\n",
      "        [-0.4327,  0.8906, -0.0493, -0.1182,  1.0996]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "--------------------------------------------------\n",
      "torch.Size([10])\n",
      "tensor([ 0.0125, -0.4433,  0.3469,  0.6609,  1.0673, -0.4327,  0.8906, -0.0493,\n",
      "        -0.1182,  1.0996], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "nlm_test_2(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_language_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dimension,hidden_dimension,\n",
    "                 window_size,vocab_size):\n",
    "        \n",
    "        super(simple_language_model,self).__init__()\n",
    "        self.embedding_dimension=embedding_dimension\n",
    "        self.hidden_dimension=hidden_dimension\n",
    "        \n",
    "        self.embedding_layer=nn.Embedding(vocab_size,embedding_dimension)\n",
    "        \n",
    "        self.hidden_layer1=nn.Linear(embedding_dimension * window_size, hidden_dimension)\n",
    "        \n",
    "        self.hidden_layer2=nn.Linear(hidden_dimension,int(hidden_dimension/2))\n",
    "        \n",
    "        self.output_layer=nn.Linear(int(hidden_dimension/2),vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self,inp):\n",
    "        \n",
    "        batch_size,window_size=inp.size()\n",
    "        \n",
    "        embeddings=self.embedding_layer(inp)\n",
    "        \n",
    "        #concatenate the prefix embeddings\n",
    "        # n x k x e => n *(k*e)\n",
    "        #7 x 2 x 5  => 7 * 10\n",
    "        \n",
    "        flattened_embeddings= embeddings.view(batch_size,window_size * self.embedding_dimension)\n",
    "        \n",
    "        linear_op1=self.hidden_layer1(flattened_embeddings)\n",
    "        print(f'linear layer 1 op size : {linear_op1.size()}')\n",
    "        \n",
    "        linear_op2=self.hidden_layer2(linear_op1)\n",
    "        print(f'linear layer 2 op size : {linear_op2.size()}')\n",
    "        \n",
    "        output=self.output_layer(linear_op2)\n",
    "        print(f'output layer size : {output.size()}')\n",
    "        print('-'*50)\n",
    "        print(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_test_3=simple_language_model(embedding_dimension=5,hidden_dimension=14,\n",
    "                               window_size=2,vocab_size=len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer 1 op size : torch.Size([7, 14])\n",
      "linear layer 2 op size : torch.Size([7, 7])\n",
      "output layer size : torch.Size([7, 18])\n",
      "--------------------------------------------------\n",
      "tensor([[-1.6439e-01,  9.9236e-02,  2.9029e-01,  4.6701e-01, -3.1879e-01,\n",
      "         -2.6406e-01, -1.1384e-01, -4.8852e-01,  2.3376e-01,  2.6768e-01,\n",
      "         -1.4251e-01, -9.6536e-02, -4.2786e-01, -4.8078e-02,  1.7719e-01,\n",
      "         -3.7736e-01, -1.7406e-01,  2.1032e-01],\n",
      "        [-5.2857e-01,  1.6246e-01,  1.2657e-01,  5.1169e-01, -2.3235e-01,\n",
      "         -2.8238e-01, -1.9139e-01, -4.1641e-01,  1.8845e-01,  7.9509e-02,\n",
      "         -4.5540e-01, -2.7618e-01, -1.3463e-01, -5.9808e-04,  5.3274e-01,\n",
      "         -6.6439e-01, -2.5536e-01,  2.3669e-01],\n",
      "        [-1.3223e-02, -2.8271e-02,  4.6956e-01,  3.8507e-01, -2.7303e-01,\n",
      "         -3.9297e-01, -1.6618e-01, -4.5953e-01,  2.9752e-01,  1.5953e-01,\n",
      "         -2.2679e-01, -1.8754e-01, -4.3761e-01, -1.7526e-01,  1.7318e-01,\n",
      "         -3.4019e-01, -1.5135e-01,  1.7221e-01],\n",
      "        [-6.9574e-02,  8.2979e-02,  2.3815e-01,  4.6584e-01, -2.5813e-01,\n",
      "         -2.5523e-01, -2.1748e-01, -2.8623e-01,  2.9386e-01,  2.8123e-01,\n",
      "          6.2738e-02,  1.5854e-02, -3.6566e-01,  2.5863e-02,  9.5054e-02,\n",
      "         -3.6307e-01, -1.6129e-01,  2.5061e-01],\n",
      "        [-2.3481e-01,  1.4352e-01,  2.8163e-01,  4.4135e-01, -2.8573e-01,\n",
      "         -9.6909e-02, -1.6574e-01, -2.2711e-01,  1.9160e-01,  1.0683e-01,\n",
      "         -1.5953e-01, -1.8089e-01, -3.5588e-01, -1.1824e-02,  3.1847e-01,\n",
      "         -4.0303e-01, -1.8512e-01,  2.5348e-01],\n",
      "        [-5.2857e-01,  1.6246e-01,  1.2657e-01,  5.1169e-01, -2.3235e-01,\n",
      "         -2.8238e-01, -1.9139e-01, -4.1641e-01,  1.8845e-01,  7.9509e-02,\n",
      "         -4.5540e-01, -2.7618e-01, -1.3463e-01, -5.9808e-04,  5.3274e-01,\n",
      "         -6.6439e-01, -2.5536e-01,  2.3669e-01],\n",
      "        [-2.0573e-01,  2.1702e-01,  2.8988e-01,  4.2862e-01, -3.6499e-01,\n",
      "         -1.4081e-01, -1.5821e-01, -2.6445e-01,  1.7399e-01,  2.4417e-01,\n",
      "         -1.9673e-01, -7.3958e-02, -5.3119e-01,  3.5425e-02,  3.1588e-01,\n",
      "         -4.1340e-01, -1.6637e-01,  1.7941e-01]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "nlm_test_3(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_language_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dimension,hidden_dimension,\n",
    "                 window_size,vocab_size):\n",
    "        \n",
    "        super(simple_language_model,self).__init__()\n",
    "        self.embedding_dimension=embedding_dimension\n",
    "        self.hidden_dimension=hidden_dimension\n",
    "        \n",
    "        self.embedding_layer=nn.Embedding(vocab_size,embedding_dimension)\n",
    "        \n",
    "        self.hidden_layer1=nn.Linear(embedding_dimension * window_size, hidden_dimension)\n",
    "        \n",
    "        self.hidden_layer2=nn.Linear(hidden_dimension,int(hidden_dimension/2))\n",
    "        \n",
    "        self.output_layer=nn.Linear(int(hidden_dimension/2),vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self,inp):\n",
    "        \n",
    "        batch_size,window_size=inp.size()\n",
    "        \n",
    "        embeddings=self.embedding_layer(inp)\n",
    "        \n",
    "        #concatenate the prefix embeddings\n",
    "        # n x k x e => n *(k*e)\n",
    "        #7 x 2 x 5  => 7 * 10\n",
    "        \n",
    "        flattened_embeddings= embeddings.view(batch_size,window_size * self.embedding_dimension)\n",
    "        \n",
    "        linear_op1=self.hidden_layer1(flattened_embeddings)\n",
    "        \n",
    "        linear_op2=self.hidden_layer2(linear_op1)\n",
    "        \n",
    "        output=self.output_layer(linear_op2)\n",
    "        \n",
    "        probabilities=f.softmax(output,dim=1)\n",
    "        print(probabilities)\n",
    "        print('-'*50)\n",
    "        print(f'sum of softmax ops (should be 1) : {probabilities.sum(dim=1)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_test_4=simple_language_model(embedding_dimension=5,hidden_dimension=14,\n",
    "                               window_size=2,vocab_size=len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0617, 0.0536, 0.0667, 0.0513, 0.0442, 0.0901, 0.0585, 0.0395, 0.0525,\n",
      "         0.0487, 0.0412, 0.0484, 0.0481, 0.0661, 0.0710, 0.0532, 0.0625, 0.0427],\n",
      "        [0.0671, 0.0643, 0.0672, 0.0453, 0.0510, 0.0963, 0.0670, 0.0483, 0.0524,\n",
      "         0.0399, 0.0462, 0.0531, 0.0526, 0.0441, 0.0487, 0.0631, 0.0551, 0.0386],\n",
      "        [0.0664, 0.0531, 0.0852, 0.0437, 0.0430, 0.0613, 0.0585, 0.0517, 0.0427,\n",
      "         0.0385, 0.0751, 0.0405, 0.0694, 0.0483, 0.0330, 0.0566, 0.0833, 0.0498],\n",
      "        [0.0698, 0.0621, 0.0784, 0.0424, 0.0478, 0.0770, 0.0652, 0.0541, 0.0476,\n",
      "         0.0355, 0.0653, 0.0464, 0.0658, 0.0383, 0.0335, 0.0631, 0.0648, 0.0430],\n",
      "        [0.0721, 0.0648, 0.0808, 0.0341, 0.0549, 0.0819, 0.0681, 0.0580, 0.0440,\n",
      "         0.0300, 0.0693, 0.0526, 0.0644, 0.0320, 0.0234, 0.0702, 0.0590, 0.0402],\n",
      "        [0.0671, 0.0643, 0.0672, 0.0453, 0.0510, 0.0963, 0.0670, 0.0483, 0.0524,\n",
      "         0.0399, 0.0462, 0.0531, 0.0526, 0.0441, 0.0487, 0.0631, 0.0551, 0.0386],\n",
      "        [0.0659, 0.0469, 0.0747, 0.0665, 0.0326, 0.0755, 0.0628, 0.0411, 0.0554,\n",
      "         0.0415, 0.0523, 0.0372, 0.0706, 0.0430, 0.0689, 0.0528, 0.0644, 0.0479]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "--------------------------------------------------\n",
      "sum of softmax ops (should be 1) : tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "nlm_test_4(prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final model and loss functions,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_language_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dimension,hidden_dimension,\n",
    "                 window_size,vocab_size):\n",
    "        \n",
    "        super(simple_language_model,self).__init__()\n",
    "        self.embedding_dimension=embedding_dimension\n",
    "        self.hidden_dimension=hidden_dimension\n",
    "        \n",
    "        self.embedding_layer=nn.Embedding(vocab_size,embedding_dimension)\n",
    "        \n",
    "        self.hidden_layer1=nn.Linear(embedding_dimension * window_size, hidden_dimension)\n",
    "        \n",
    "        self.hidden_layer2=nn.Linear(hidden_dimension,int(hidden_dimension/2))\n",
    "        \n",
    "        self.output_layer=nn.Linear(int(hidden_dimension/2),vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self,inp):\n",
    "        \n",
    "        batch_size,window_size=inp.size()\n",
    "        \n",
    "        embeddings=self.embedding_layer(inp)\n",
    "        \n",
    "        #concatenate the prefix embeddings\n",
    "        # n x k x e => n *(k*e)\n",
    "        #7 x 2 x 5  => 7 * 10\n",
    "        \n",
    "        flattened_embeddings= embeddings.view(batch_size,window_size * self.embedding_dimension)\n",
    "        \n",
    "        linear_op1=self.hidden_layer1(flattened_embeddings)\n",
    "        \n",
    "        linear_op2=self.hidden_layer2(linear_op1)\n",
    "        \n",
    "        output=self.output_layer(linear_op2)\n",
    "        \n",
    "        return output #logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_model=simple_language_model(embedding_dimension=5,hidden_dimension=14,\n",
    "                               window_size=2,vocab_size=len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=90\n",
    "learning_rate= 0.1\n",
    "\n",
    "\n",
    "loss_criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(params=nlm_model.parameters(),\n",
    "                         lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/90, Loss 2.9957337379455566\n",
      "Epoch 1/90, Loss 2.9441401958465576\n",
      "Epoch 2/90, Loss 2.8939268589019775\n",
      "Epoch 3/90, Loss 2.8444085121154785\n",
      "Epoch 4/90, Loss 2.794975757598877\n",
      "Epoch 5/90, Loss 2.7450692653656006\n",
      "Epoch 6/90, Loss 2.694162607192993\n",
      "Epoch 7/90, Loss 2.6417553424835205\n",
      "Epoch 8/90, Loss 2.5873589515686035\n",
      "Epoch 9/90, Loss 2.530500888824463\n",
      "Epoch 10/90, Loss 2.47072434425354\n",
      "Epoch 11/90, Loss 2.407597780227661\n",
      "Epoch 12/90, Loss 2.340738534927368\n",
      "Epoch 13/90, Loss 2.269840717315674\n",
      "Epoch 14/90, Loss 2.19472336769104\n",
      "Epoch 15/90, Loss 2.115389347076416\n",
      "Epoch 16/90, Loss 2.0320916175842285\n",
      "Epoch 17/90, Loss 1.9453915357589722\n",
      "Epoch 18/90, Loss 1.8561879396438599\n",
      "Epoch 19/90, Loss 1.7656736373901367\n",
      "Epoch 20/90, Loss 1.6752084493637085\n",
      "Epoch 21/90, Loss 1.5861200094223022\n",
      "Epoch 22/90, Loss 1.4994993209838867\n",
      "Epoch 23/90, Loss 1.4160875082015991\n",
      "Epoch 24/90, Loss 1.3362902402877808\n",
      "Epoch 25/90, Loss 1.2602933645248413\n",
      "Epoch 26/90, Loss 1.1881943941116333\n",
      "Epoch 27/90, Loss 1.1200941801071167\n",
      "Epoch 28/90, Loss 1.0561292171478271\n",
      "Epoch 29/90, Loss 0.9964560270309448\n",
      "Epoch 30/90, Loss 0.9412079453468323\n",
      "Epoch 31/90, Loss 0.8904477953910828\n",
      "Epoch 32/90, Loss 0.8441299200057983\n",
      "Epoch 33/90, Loss 0.8020862340927124\n",
      "Epoch 34/90, Loss 0.7640341520309448\n",
      "Epoch 35/90, Loss 0.7296029925346375\n",
      "Epoch 36/90, Loss 0.6983703374862671\n",
      "Epoch 37/90, Loss 0.6698991656303406\n",
      "Epoch 38/90, Loss 0.6437711119651794\n",
      "Epoch 39/90, Loss 0.6196085810661316\n",
      "Epoch 40/90, Loss 0.5970895886421204\n",
      "Epoch 41/90, Loss 0.5759496092796326\n",
      "Epoch 42/90, Loss 0.5559806823730469\n",
      "Epoch 43/90, Loss 0.5370232462882996\n",
      "Epoch 44/90, Loss 0.5189594030380249\n",
      "Epoch 45/90, Loss 0.5017043948173523\n",
      "Epoch 46/90, Loss 0.4852001368999481\n",
      "Epoch 47/90, Loss 0.469408243894577\n",
      "Epoch 48/90, Loss 0.4543052315711975\n",
      "Epoch 49/90, Loss 0.43987783789634705\n",
      "Epoch 50/90, Loss 0.4261191189289093\n",
      "Epoch 51/90, Loss 0.4130255877971649\n",
      "Epoch 52/90, Loss 0.40059447288513184\n",
      "Epoch 53/90, Loss 0.38882192969322205\n",
      "Epoch 54/90, Loss 0.3777018189430237\n",
      "Epoch 55/90, Loss 0.36722442507743835\n",
      "Epoch 56/90, Loss 0.35737648606300354\n",
      "Epoch 57/90, Loss 0.3481411039829254\n",
      "Epoch 58/90, Loss 0.339497834444046\n",
      "Epoch 59/90, Loss 0.3314233124256134\n",
      "Epoch 60/90, Loss 0.32389160990715027\n",
      "Epoch 61/90, Loss 0.316875159740448\n",
      "Epoch 62/90, Loss 0.3103451430797577\n",
      "Epoch 63/90, Loss 0.3042721748352051\n",
      "Epoch 64/90, Loss 0.298627108335495\n",
      "Epoch 65/90, Loss 0.2933809161186218\n",
      "Epoch 66/90, Loss 0.2885056138038635\n",
      "Epoch 67/90, Loss 0.2839741110801697\n",
      "Epoch 68/90, Loss 0.27976080775260925\n",
      "Epoch 69/90, Loss 0.275841623544693\n",
      "Epoch 70/90, Loss 0.2721933424472809\n",
      "Epoch 71/90, Loss 0.268794983625412\n",
      "Epoch 72/90, Loss 0.2656269967556\n",
      "Epoch 73/90, Loss 0.2626707851886749\n",
      "Epoch 74/90, Loss 0.2599097788333893\n",
      "Epoch 75/90, Loss 0.2573283612728119\n",
      "Epoch 76/90, Loss 0.25491243600845337\n",
      "Epoch 77/90, Loss 0.2526490092277527\n",
      "Epoch 78/90, Loss 0.2505260109901428\n",
      "Epoch 79/90, Loss 0.24853262305259705\n",
      "Epoch 80/90, Loss 0.24665892124176025\n",
      "Epoch 81/90, Loss 0.2448958158493042\n",
      "Epoch 82/90, Loss 0.24323485791683197\n",
      "Epoch 83/90, Loss 0.2416684925556183\n",
      "Epoch 84/90, Loss 0.2401898354291916\n",
      "Epoch 85/90, Loss 0.2387925088405609\n",
      "Epoch 86/90, Loss 0.23747055232524872\n",
      "Epoch 87/90, Loss 0.23621878027915955\n",
      "Epoch 88/90, Loss 0.23503220081329346\n",
      "Epoch 89/90, Loss 0.23390625417232513\n"
     ]
    }
   ],
   "source": [
    "losses=[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    outputs=nlm_model(prefixes)\n",
    "    \n",
    "    loss=loss_criterion(outputs,labels)\n",
    "    \n",
    "    #compute gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    #backpropagate and flush out the gradients for next epoch\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f'Epoch {epoch}/{num_epochs}, Loss {loss.item()}')\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Num Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx to word\n",
    "reverse_vocabulary = dict((idx, word )  for (word,idx) in vocabulary.items()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Rishab', 1: 'likes', 2: 'pizza', 3: 'I', 4: 'love', 5: 'gaming', 6: 'Divyansh', 7: 'loves', 8: 'coding', 9: 'NLP', 10: 'is', 11: 'fun', 12: 'Chicken', 13: 'tasty', 14: 'running', 15: 'Usain', 16: 'runs', 17: 'fast'}\n"
     ]
    }
   ],
   "source": [
    "print(reverse_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is \"Rishab likes\" , the model predicts \"pizza\" with 0.9796783924102783 probability\n"
     ]
    }
   ],
   "source": [
    "rish_likes= prefixes[0].unsqueeze(0) #to add a dummy dim for nn model\n",
    "#print(test_item.shape)\n",
    "\n",
    "test_op=nlm_model(rish_likes)\n",
    "\n",
    "probab=f.softmax(test_op,dim=1).squeeze()\n",
    "\n",
    "pred_word=torch.argmax(probab).item()\n",
    "\n",
    "print(f'Input is \"Rishab likes\" , the model predicts \"{reverse_vocabulary[pred_word]}\" with {probab[pred_word]} probability')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is \"Usain runs\" , the model predicts \"fast\" with 0.986263632774353 probability\n"
     ]
    }
   ],
   "source": [
    "usain= prefixes[6].unsqueeze(0) #to add a dummy dim for nn model\n",
    "#print(test_item.shape)\n",
    "\n",
    "test_op=nlm_model(usain)\n",
    "\n",
    "probab=f.softmax(test_op,dim=1).squeeze()\n",
    "\n",
    "pred_word=torch.argmax(probab).item()\n",
    "\n",
    "print(f'Input is \"Usain runs\" , the model predicts \"{reverse_vocabulary[pred_word]}\" with {probab[pred_word]} probability')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is \"Divyansh likes\" , the model predicts \"coding\" with 0.9468822479248047 probability\n"
     ]
    }
   ],
   "source": [
    "div= prefixes[2].unsqueeze(0) #to add a dummy dim for nn model\n",
    "#print(test_item.shape)\n",
    "\n",
    "test_op=nlm_model(div)\n",
    "\n",
    "probab=f.softmax(test_op,dim=1).squeeze()\n",
    "\n",
    "pred_word=torch.argmax(probab).item()\n",
    "\n",
    "print(f'Input is \"Divyansh likes\" , the model predicts \"{reverse_vocabulary[pred_word]}\" with {probab[pred_word]} probability')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
